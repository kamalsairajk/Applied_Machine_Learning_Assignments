{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: CNNs & RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'Kamal Sai Raj Kuncha'\n",
    "email = 'k.kuncha@ufl.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 7 -- name: Kamal Sai Raj Kuncha, email: k.kuncha@ufl.edu\n",
      "\n",
      "### Python version: 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]\n",
      "### NumPy version: 1.19.5\n",
      "### Scikit-learn version: 0.24.0\n",
      "### Tensorflow version: 2.4.0\n",
      "### TF Keras version: 2.4.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 7 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    # we'll use tensorflow and keras for neural networks\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    \n",
    "    # import layers we may use\n",
    "    from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Dropout, Activation\n",
    "\n",
    "    # import callbacks we may use\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    #%load_ext tensorboard\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('### Tensorflow version: ' + tf.__version__)\n",
    "    print('### TF Keras version: ' + keras.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (20 points) Loading and Processing CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (20 points) Complete the implementation of load_preprocess_cifar10(). Make sure you correctly implement all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "# and to https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def load_preprocess_cifar10(onehot=True, minmax_normalize=True):\n",
    "    \n",
    "    labels = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    \n",
    "    ### Load and preprocess the cifar10 data, then split it into train, test, validation\n",
    "    ### The shapes of train_x, test_x, val_x should be: (50000, 32, 32, 3), (5000, 32, 32, 3), (5000, 32, 32, 3)\n",
    "    ### If onehot=True you need to one hot encode the labels (y vector)\n",
    "    ### If minmax_normalize=True you need to minmax normalize the pixel values to be in the range [0,1]\n",
    "    ###* put your code here (~10-20 lines) *###\n",
    "    #print(keras.backend.image_data_format())#'channels_last'\n",
    "    train,testval=cifar10.load_data()\n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        train_x = train_x / 255.0\n",
    "        testval_x = testval_x / 255.0\n",
    "    \n",
    "    if onehot:\n",
    "        num_classes=10\n",
    "        train_y=keras.utils.to_categorical(train_y,num_classes)\n",
    "        testval_y=keras.utils.to_categorical(testval_y,num_classes)\n",
    "    # split test - val\n",
    "    nval = testval_x.shape[0] // 2\n",
    "    \n",
    "    val_x = testval_x[:nval]\n",
    "    val_y = testval_y[:nval]\n",
    "    \n",
    "    test_x = testval_x[nval:]\n",
    "    test_y = testval_y[nval:]\n",
    "    \n",
    "    \n",
    "#     val_x,val_y,test_x,test_y=train_test_split(test_images,test_labels,train_size=int(test_images.shape[0]/2), test_size=int(test_images.shape[0]/2))\n",
    "\n",
    "    \n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do some sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=False, minmax_normalize=False)\n",
    "assert train_x.shape[0] == train_y.shape[0] and test_x.shape[0] == test_y.shape[0] and val_x.shape[0] == val_y.shape[0]\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],) or train_y.shape == (train_y.shape[0],1)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=True, minmax_normalize=False)\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],10) and train_y.shape[1] == test_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10()\n",
    "assert np.amax(train_x) <= 1 and np.amax(test_x) <= 1 and np.amax(val_x) <= 1\n",
    "assert np.amax(train_x) >= 0 and np.amax(test_x) >= 0 and np.amax(val_x) >= 0\n",
    "\n",
    "assert labels.shape[0] == 10 and labels.shape[0] == train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Training a CNN for Cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the following architecture\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Flatten\n",
    "- FC with 128 units\n",
    "- Dropout with rate 25%\n",
    "- FC with 64 units\n",
    "- Dropout with rate 25%\n",
    "- (Output layer) FC with 10 units\n",
    "\n",
    "#### For all layers (if applicable) except the output layer you should use:\n",
    "- ReLU as activation function\n",
    "- He uniform weight initialization strategy\n",
    "- L2 regularization with regularization constant set to 0.001\n",
    "\n",
    "#### For the output layer you should select a suitable activation function that is consistent with the task and loss function you use. Use Adam for the optimizer with learning rate 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (20 points) Implement create_compile_cnn() according to the architecture specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "def create_compile_cnn(input_shape=[32, 32, 3], num_outputs=10, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='CIFAR-10--CNN')\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~20 lines) *###\n",
    "    model.add(Conv2D(filters=32, input_shape=input_shape, kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same',kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128,kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(64,kernel_initializer=initializer,kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    opt=keras.optimizers.Adam(lr=0.002)   \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (10 points) Train the model. Fill in the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "391/391 [==============================] - 161s 409ms/step - loss: 2.9364 - accuracy: 0.1872 - val_loss: 1.9072 - val_accuracy: 0.3922\n",
      "Epoch 2/15\n",
      "391/391 [==============================] - 158s 404ms/step - loss: 1.8720 - accuracy: 0.3782 - val_loss: 1.5613 - val_accuracy: 0.4890\n",
      "Epoch 3/15\n",
      "391/391 [==============================] - 164s 419ms/step - loss: 1.6365 - accuracy: 0.4682 - val_loss: 1.4081 - val_accuracy: 0.5536\n",
      "Epoch 4/15\n",
      "391/391 [==============================] - 155s 396ms/step - loss: 1.4982 - accuracy: 0.5263 - val_loss: 1.3842 - val_accuracy: 0.5656\n",
      "Epoch 5/15\n",
      "391/391 [==============================] - 164s 419ms/step - loss: 1.4241 - accuracy: 0.5624 - val_loss: 1.2342 - val_accuracy: 0.6330\n",
      "Epoch 6/15\n",
      "391/391 [==============================] - 164s 420ms/step - loss: 1.3608 - accuracy: 0.5878 - val_loss: 1.1739 - val_accuracy: 0.6606\n",
      "Epoch 7/15\n",
      "391/391 [==============================] - 157s 403ms/step - loss: 1.3207 - accuracy: 0.6080 - val_loss: 1.1892 - val_accuracy: 0.6602\n",
      "Epoch 8/15\n",
      "391/391 [==============================] - 161s 411ms/step - loss: 1.2943 - accuracy: 0.6266 - val_loss: 1.0982 - val_accuracy: 0.6922\n",
      "Epoch 9/15\n",
      "391/391 [==============================] - 165s 421ms/step - loss: 1.2645 - accuracy: 0.6399 - val_loss: 1.1978 - val_accuracy: 0.6574\n",
      "Epoch 10/15\n",
      "391/391 [==============================] - 163s 418ms/step - loss: 1.2476 - accuracy: 0.6464 - val_loss: 1.1310 - val_accuracy: 0.6790\n",
      "Epoch 11/15\n",
      "391/391 [==============================] - 164s 420ms/step - loss: 1.2349 - accuracy: 0.6516 - val_loss: 1.0527 - val_accuracy: 0.7158\n",
      "Epoch 12/15\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 1.2126 - accuracy: 0.6644 - val_loss: 1.0480 - val_accuracy: 0.7190\n",
      "Epoch 13/15\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 1.1971 - accuracy: 0.6757 - val_loss: 1.0468 - val_accuracy: 0.7248\n",
      "Epoch 14/15\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 1.1933 - accuracy: 0.6717 - val_loss: 1.0918 - val_accuracy: 0.7054\n",
      "Epoch 15/15\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 1.1782 - accuracy: 0.6787 - val_loss: 1.0177 - val_accuracy: 0.7286\n"
     ]
    }
   ],
   "source": [
    "cnn_model_fp = './cifar10-cnn.h5'\n",
    "\n",
    "# If the model file exists, load it. Otherwise train it and save the model.\n",
    "# Note: if you need to retrain the model, simply delete the h5 file.\n",
    "if os.path.exists(cnn_model_fp):\n",
    "    model = keras.models.load_model(cnn_model_fp)\n",
    "else:\n",
    "    model = create_compile_cnn(verbose=False)\n",
    "    \n",
    "    # train the model using model.fit() for at least 3 epochs and your chosen batch_size\n",
    "    # you can set any callback you want on it, including checkpoint, early stopping, etc.\n",
    "    ###* put your code here (~3-5 lines) *###\n",
    "    early_stop_cb = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "    checkpoint_cb = ModelCheckpoint(cnn_model_fp, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    max_epochs=15\n",
    "    batch_size=128\n",
    "    model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, shuffle=True, callbacks=[early_stop_cb,checkpoint_cb])\n",
    "    \n",
    "    \n",
    "    # save the model\n",
    "    model.save(cnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 72.06%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (15 points) Processing Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (15 points) Fill in the implementation of load_preprocess_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# the size of the vocabulary we'll use\n",
    "vocab_size = 12000\n",
    "maxlen = 150\n",
    "\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "# and to https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def load_preprocess_imdb(num_words=vocab_size, prop_vec=prop_vec, maxlen=maxlen, vectorize=False):\n",
    "    \n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    \n",
    "    train, testval = imdb.load_data(num_words=num_words, maxlen=maxlen, oov_char=0)\n",
    "    \n",
    "    np.warnings.filterwarnings('default', category=np.VisibleDeprecationWarning)    \n",
    "    \n",
    "    ### Process the data \n",
    "    ### Merge train and testval, but then split again into train, test, val sets (according to prop_vec). You can use utils.train_test_val_split().)\n",
    "    ### - If vectorize=True, then you must encode the features of each example into vectors of vocab_size entries \n",
    "    ### such that entry i contains the number of time word i appeared in the sequence\n",
    "    ### - If vectorize=False, then you must encode the features of each examples as a sequence of size maxlen (represented as a np.array()). \n",
    "    ### Make sure to pad sequences with 0 as appropriate.\n",
    "    ###* put your code here (~10-15 lines) *###\n",
    "    x=np.concatenate((train[0],testval[0]))\n",
    "    y=np.concatenate((train[1],testval[1]))\n",
    "    if vectorize:\n",
    "        new_x=np.zeros((len(x),vocab_size))\n",
    "        for (i,j) in enumerate(x):\n",
    "            for word_num in j:\n",
    "                new_x[i,word_num]+=1\n",
    "        x=new_x\n",
    "    else:\n",
    "        x=tf.keras.preprocessing.sequence.pad_sequences(x,maxlen=maxlen)\n",
    "    \n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(x, y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)\n",
    "assert train_x.shape == (16281, maxlen) and train_y.shape == (train_x.shape[0],)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)\n",
    "assert train_x.shape == (16281, vocab_size) and train_y.shape == (train_x.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (35 points) RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (35 points) Complete the code below to define an RNN architecture for sentiment analysis. The goal is to predict the sentiment of IMDB reviews. You can use any architecture you want, but a good place to start would be to use an embedding layer followed by some recurrent layers (e.g., LSTM, GRU, etc.). Keep the number of parameters of the model below 2m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "def create_compile_rnn(input_shape=[None], vocab_size=vocab_size, embedding_size=128, num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-RNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ### Use binary_crossentropy as loss function.    \n",
    "    ###* put your code here (~15-20 lines) *###\n",
    "\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=150))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "    if(verbose):\n",
    "        model.summary()\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 150, 128)          1536000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,627,701\n",
      "Trainable params: 1,627,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_rnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "509/509 [==============================] - 59s 112ms/step - loss: 0.5690 - accuracy: 0.6805 - val_loss: 0.3201 - val_accuracy: 0.8702\n",
      "Epoch 2/3\n",
      "509/509 [==============================] - 54s 105ms/step - loss: 0.2187 - accuracy: 0.9216 - val_loss: 0.3221 - val_accuracy: 0.8761\n",
      "Epoch 3/3\n",
      "509/509 [==============================] - 59s 116ms/step - loss: 0.1304 - accuracy: 0.9556 - val_loss: 0.3581 - val_accuracy: 0.8695\n"
     ]
    }
   ],
   "source": [
    "# let's load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)\n",
    "\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 88.43%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): DNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous task, we use an RNN for sentiment analysis. In this task you will use a neural network without any recurrent layers for the same task as a comparison.\n",
    "\n",
    "### We'll use the data in vectorized form for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (20 points) Complete the code below to define an architecture of your choice *without* any recurrent layers. The goal is to get the best model with the fewest number of parameters. Keep the number of parameters of the model below 2m and ideally similar to the model of Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_dnn(input_shape=[vocab_size], num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-DNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~10 lines) *###\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape, sparse=False, name='input')) \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if(verbose):\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 128)               1536128   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,546,497\n",
      "Trainable params: 1,546,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_dnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 7s 35ms/step - loss: 0.4858 - accuracy: 0.7535 - val_loss: 0.2795 - val_accuracy: 0.8894\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 0.1685 - accuracy: 0.9384 - val_loss: 0.2976 - val_accuracy: 0.8761\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 0.0834 - accuracy: 0.9730 - val_loss: 0.3513 - val_accuracy: 0.8776\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.4551 - val_accuracy: 0.8754\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.6347 - val_accuracy: 0.8650\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 0.0179 - accuracy: 0.9938 - val_loss: 0.6002 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "# Let's load the data in vectorized form\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)\n",
    "\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size,validation_data=(val_x, val_y), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 89.98%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (5 points) Compare this model to the model of Task 4. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "# I am able to get similar accuracy to the model of Task 4. But, I think that the model is over fitting as the validation \n",
    "#  loss for my dnn model was increasing.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
